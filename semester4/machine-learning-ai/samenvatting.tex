\documentclass{article}

\usepackage[dutch]{babel}
\usepackage[margin=3cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage[parfill]{parskip}

% fonts
\usepackage[T1]{fontenc}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

\graphicspath{{img/}}

% theorem environment
\usepackage{amssymb}

\newtheorem{theorem}{Definitie}[section]

\usepackage{enumitem}

\newenvironment{thmenum}
 {\begin{enumerate}[label=\upshape\bfseries(\roman*)]}
 {\end{enumerate}}


% code
\usepackage{minted}
\setminted{frame=single,framesep=3pt,linenos}
\usepackage{upquote}
\usepackage{color}

\begin{document}

\begin{titlepage}
    \author{Tuur Vanhoutte}
    \title{Machine Learning \& AI}
\end{titlepage}

\pagenumbering{gobble}
\maketitle
\newpage
\tableofcontents
\newpage

\pagenumbering{arabic}

\section{Inleiding}

\subsection{AI in context}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{ai-history.png}
    \caption{Geschiedenis van AI}
\end{figure}

Belangrijkste gebeurtenissen:

\begin{itemize}
    \item \textbf{1943:} McCulloch - Pitts: fundering van neurale netwerken
    \item \textbf{1950:} Alan Turing: de Turing test
    \item \textbf{1956:} Dartmouth workshop: bijeenkomst voor breinstorm AI
    \item \textbf{1997:} Garry Kasparov vs Deep Blue (IBM)
    \item \textbf{2011:} IBM Watson
    \item \textbf{2016:} AlphaGo
    \item \textbf{2021-:} toekomst
\end{itemize}

\subsubsection{Vormen van AI}

\begin{itemize}
    \item Zwakke AI (weak AI / Artificial Narrow Intelligence)
    \begin{itemize}
        \item Goed in een bepaalde taak maar alleen in die taak
        \item \textbf{Voorbeelden: } spamfilters, schaakcomputers, gezichtsherkenning
    \end{itemize}
    \item Sterke AI (strong AI / Artificial General Intelligence)
    \begin{itemize}
        \item Intelligentie op menselijk niveau
        \item In staat om zich aan te passen en problemen te leren oplossen in verschillende contexten
    \end{itemize}
    \item Superintelligentie (Artificial Super Intelligence)
    \begin{itemize}
        \item Als AI zelfbewust wordt en de mens op alle vlakken voorbij steekt
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{ai-history2.png}
    \caption{AI vs ML vs DL}
\end{figure}

\subsubsection{Sectoren die de planeet verbeteren}

\begin{itemize}
    \item Klimaatsverandering
    \item Biodiversiteit en conservatie
    \item Water
    \item Hernieuwbare energie
    \item Medische sector
    \item Weer- en rampenvoorspelling
\end{itemize}

\subsubsection{Waarom nu?}

\begin{itemize}
    \item Snellere hardware
    \item Betere algoritmes
    \item Meer data
    \item (Open source) frameworks
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{nvidia-tesla.png}
    \caption{Voorbeeld huidige hardware: de Tesla V100 van Nvidia}
\end{figure}

\section{Hoe leren uit data?}

\subsection{Leeralgoritmes}

\begin{itemize}
    \item Supervised
    \begin{itemize}
        \item Inputs met gewenste outputs zijn gegeven
        \item Task driven
    \end{itemize}
    \item Unsupervised
    \begin{itemize}
        \item De gewenste outputs zijn niet gegeven
        \item Data driven (clustering)
    \end{itemize}
    \item Reinforcement
    \begin{itemize}
        \item Beslissingsproces op basis van beloningen
        \item Algoritme leert te reageren op zijn omgeving
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{leeralgoritmes.png}
    \caption{Supervised / Unsupervised / Reinforcement learning}
\end{figure}


\subsection{Supervised Learning}

Leren uit een gelabelde dataset. Vind het verband tussen de features en de labels

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{supervised-learning.png}
    \includegraphics[width=0.4\textwidth]{supervised-learning2.png}
    \caption{Leren uit een dataset}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{supervised-learning3.png}
    \caption{Supervised learning kan uit ongeziene data een resultaat berekenen}
\end{figure}

\subsubsection{Regressie vs Classificatie}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{regressie-vs-classificatie.png}
    \caption{Regressie vs classificatie}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{regressie-vs-classificatie2.png}
    \caption{Regressie vs classificatie}
\end{figure}

\subsubsection{Voorbeeld}

Hoe stuurhoek bepalen bij een self-driving car?

\begin{itemize}
    \item (infrarood) camera's
    \item Stereo vision
    \item Radar
    \item LIDAR
    \item GPS
    \item Audio
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{stuurhoek-selfdriving-car.png}
    \includegraphics[width=0.3\textwidth]{stuurhoek-selfdriving-car2.png}
    \caption{Via sensoren weet de auto }
\end{figure}



\subsection{Unsupervised learning}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{unsupervised-learning.png}
    \caption{Unsupervised Learning}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{unsupervised-learning2.png}
    \caption{Voorbeeld Clustering: de data in groepen verdelen}
\end{figure}

\subsection{Reinforcement learning}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{reinforcement-learning.png}
    \caption{Reinforcement learning}
\end{figure}

\begin{itemize}
    \item Voor elke actie krijgt de AI feedback
    \item De AI leert uit de feedback
    \item In het begin zijn de acties heel willekeurig
\end{itemize}

\subsection{Overzicht leeralgoritmes}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{overzicht-leeralgoritmes.png}
    \includegraphics[width=0.5\textwidth]{overzicht-leeralgoritmes2.png}
    \caption{Overzicht}
\end{figure}

\subsection{Werkwijze van een ML Project}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{machine-learning-project.png}
    \includegraphics[width=0.35\textwidth]{machine-learning-project2.png}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{machine-learning-project3.png}
    \caption{}
\end{figure}

\subsubsection{Tijdverdeling}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{machine-learning-project4.png}
    \caption{Tijdverdeling: verwachting vs realiteit}
\end{figure}

\section{Enkelvoudige Lineaire regressie}

\subsection{Voorbeeld}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{lineaire-regressie-voorbeeld.png}
    \caption{Voorspel de bloeddruk op basis van leeftijd en gewicht}
\end{figure}

\begin{itemize}
    \item \textbf{features:} leeftijd en gewicht
    \item \textbf{target:} bloeddruk (=wat je wil voorspellen = output = label)
    \item \textbf{trainingset:} 11 training examples (=samples)
\end{itemize}

\begin{theorem}[Regressie-analyse]
Regressie-analyse is het modelleren van of het zoeken naar een verband op basis van één of meerdere variabelen.

Bij regressie is de output/target een (continue) variabele
\end{theorem}

\subsubsection{Scatterplot}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{lineaire-regressie-voorbeeld-scatterplot.png}
    \caption{Scatterplot: de grafiek toont een positieve correlatie $\Rightarrow$ een sterk verband}
\end{figure}

\subsection{De hypothese}

\begin{theorem}[De hypothese]
Het verband (model of hypothese) $h_{\theta}(x)$ is van de vorm:

\begin{equation}
h_{\theta}(x) = \theta_0 + \theta_1x
\end{equation}
\end{theorem}

Bepalen van de optimale waarden voor $\theta_0$ en $\theta_1$:

\begin{itemize}
    \item $\theta_0$ = snijpunt van de y-as (= noemen we ook de \textbf{bias})
    \item $\theta_1$ = helling van de rechte (rico)
\end{itemize}

De parameters $\theta_i$ = \textbf{weights}

Het zoeken van het model / hypothese = \textbf{training / learning}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{lineaire-regressie-hypothese.png}
    \caption{Lineaire trnedlijn met model $h_{\theta}(x)$}
\end{figure}

$\mathbf{R^2}$\textbf{-waarde:} determinatiecoëfficiënt

\subsection{De kostenfunctie}

We moeten de kostenfunctie $J(\theta)$ via de \textbf{Least Mean Squared} methode (LMS). 

\begin{equation}
J(\theta) = \frac{1}{2\cdot m} \cdot \sum_{i=1}^m (h_{\theta}(x_i) - y_i)^2
\end{equation}

\begin{itemize}
    \item $m =$ de bias $=$ snijpunt met de y-as
    \item De kostenfunctie berekent de gemiddelde fout door alle fouten op te tellen
    \item Elke fout wordt gekwadrateerd om:
    \begin{itemize}
        \item negatieve waardes positief te maken
        \item de fout uit te groten
    \end{itemize}
\end{itemize}

\subsection{Gradient Descent (GDS)}

\begin{equation}
J(\theta_0, \theta_1) = \frac{1}{2\cdot m} \cdot \sum_{i=1}^m ((\theta_1\cdot x_i + \theta_0) - y_i)^2
\end{equation}

Stel de parameters $\theta_0$ en $\theta_1$ voortdurend bij in een iteratief proces 
tot je de waarden voor $\theta_0$ en $\theta_1$ hebt gevonden die de kleinst mogelijke waarde. Start met willekeurige waarden.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{gradient-descent-theta0.png}
    \caption{De GDS als $\theta_0 = 0$}
\end{figure}

\begin{itemize}
    \item Je krijgt een dalparabool als uitkomst 
    \item Je kan aflezen wat de parameters moeten zijn om de kleinst mogelijke waarde te vinden
    \item In de realiteit heb je vaak veel meer dan 2 gewichten
    \begin{itemize}
        \item Voorbeeld: de textgenererende AI GPT-3 heeft rond de 175 miljard gewichten 
        \item $\Rightarrow$ veel rekenkracht nodig om beste uitkomst te vinden
    \end{itemize}
\end{itemize}

\subsubsection{Learning rate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{gradient-descent-afgeleiden.png}
    \caption{De parameters $\theta_0$ en $\theta_1$ stellen we constant bij (formules niet te kennen)}
\end{figure}

\begin{itemize}
    \item We bepalen de afgeleide (= de gradient, de helling) van $\theta_0$ en $\theta_1$
    \item We gebruiken die afgeleiden om een betere $\theta_0$ en $\theta_1$ te vinden.
    \item We vermenigvuldigen de gradient met een variable $\eta$ (=de learning rate)
    \item Onze nieuwe $\theta$ wordt berekend met behulp van de oude $\theta$ en de afgeleide maal de learning rate. 
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{lineaire-regressie-learning-rate.png}
    \caption{Learning rate $\eta$: bij een te kleine/te grote $\eta$ hebben we te veel stappen}
\end{figure}

De learning rate $\eta$ stellen we constant bij om met zo weinig aantal stappen het optimum te bereiken.


\section{Meervoudige lineaire regressie}

\begin{theorem}[Meervoudige lineaire regressie]
Bij meervoudige lineaire regressie (multiple regression) wordt het model/hypothese bepaald 
aan de hand van een trainingset met \textbf{meerdere features}.
\end{theorem}

\begin{itemize}
    \item Bloeddruk wordt bepaald a.d.h.v. gewicht en leeftijd
    \item De kwaliteit van wijn wordt voorspeld op basis van: zuurtegraad, suikergehalte, chloriden, dichtheid, sulfaten, hoeveelheid alcohol, \dots
    \item Het warmeverlies van een huis wordt voorspeld op basis van: het type glas, muurisolatie, oriëntatie van het huis, \dots
\end{itemize}

\begin{equation}
h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n + 
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{multiple-regression-voorbeeld.png}
    \caption{\textbf{Voorbeeld:} voorspel de huisprijs op basis van deze features }
\end{figure}

\subsection{Statistische vooranalyse}

\subsubsection{Consistentie van de dataset}

\begin{itemize}
    \item Volledigheid van de dataset
    \item Inconsistenties
    \item Spreiding van de gegevens
\end{itemize}

We verwijderen de CHAS kolom:

\begin{minted}{python}
dataset.drop('CHAS', axis=1, inplace=True)
\end{minted}


\subsubsection{Uitschieters}

\begin{itemize}
    \item Vinden en verwijderen van extreme waarden/samples
    \item Geavanceerde technieken: zie later bij clustering
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multiple-regression-uitschieters.png}
    \caption{Uitschieters}
\end{figure}


\subsubsection{Onderlinge correlatie}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{multiple-regression-heatmap.png}
    \includegraphics[width=0.45\textwidth]{multiple-regression-pairplot.png}
    \caption{Heatmap en pairplot van de onderline correlatie tussen de features}
\end{figure}

\subsection{Features en targets}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multiple-regression-features-targets.png}
    \caption{Dataset opsplitsen in features en targets}
\end{figure}

\begin{minted}{python}
y = dataset['target_kolom'].values
X = dataset.drop('target_kolom',axis=1).values
# Alternatief
features=list(dataset.columns[:dataset.columns.size-1])
X = dataset[features].values
y = dataset['Price'].values
\end{minted}

\subsection{Trainen van het model}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multiple-regression-training-testset.png}
    \caption{Dataset opsplitsen in training set en test set}
\end{figure}

\begin{itemize}
    \item Belangrijk om eerst de data te randomiseren: te data zou gesorteerd kunnen zijn, dat willen we vermijden
    \item Stel dat huizenprijzen van laag naar hoog gesorteerd is, en je traint de data op de eerste 75\%, en test de laatste 25\%.
    Resultaat: Het model zal niet getraind zijn op dure huizen.
    \item Ander voorbeeld: stel dat je een self-driving AI alleen traint op de autosnelweg, en dan test in een zone-30 straat bij een school...
\end{itemize}


\begin{minted}{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, 
random_state=0)
\end{minted}

\subsubsection{Initialiseren en trainen van het regressiemodel}

\begin{minted}{python}
lregmodel = linear_model.LinearRegression()
lregmodel.fit(X_train, y_train)
\end{minted}

Model:

\begin{minted}{python}
print('coeffs: ', lregmodel.coef_)
print('intercept', lregmodel.intercept_)

>> coeffs: [ -3.56141289e+00, 4.05479295 e-01, 8.14080284 e-01, 
    8.96450415 e+01, -3.02997261e-01, -2.77339444e+01, 
    7.47151897 e+00, -2.92233040e-01, -1.61741146e+01, 
    -1.17962045e +01]

>> intercept: 650.652022517
\end{minted}

Price = - 3.56 × CRIM + 0.41 × ZN + 0.81 × INDUS - 270.51 × NOX + 89.65 × RM
- 0.30 × AGE - 27.74 × DIS + 7.47 × RAD - 0.29 × TAX - 16, 17 × PT
+ 0.08 × B - 11.80 × LSTAT + 650.65


\subsection{Testen van het model}

\subsubsection{Voorspellingen maken}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multiple-regression-voorspelling.png}
    \caption{Voorspel de prijs van een huis met deze features}
\end{figure}

\begin{minted}{python}
house = np.array([0.11, 0, 12.03, 0.57, 6.80, 89.30, 2.39, 1, 
273, 21.00, 393.45, 6.48])

house = house.reshape(1, -1)

# met reshape wordt house:
# house = np.array([[0.11, 0, 12.03, 0.57, 6.80, 89.30, 2.39, 1, 
# 273, 21.00, 393.45, 6.48]])

price = lregmodel.predict(house)

print('De prijs van het huis bedraagt: ', price)

>> De prijs van het huis bedraagt: 563.68335073
\end{minted}

\begin{itemize}
    \item reshape(1, -1) maakt een rijvector
    \item Werkelijke prijs: 562.00
\end{itemize}

\subsection{Performantie en scores van het model}

\subsubsection{Mean Absolute Error (MAE)}

\begin{theorem}[MAE]
De Mean Absolute Error (MAE) is \textbf{het gemiddelde van de absolute waarden} van het verschil
tussen de \textbf{werkelijke waarden} $y_i$ en de \textbf{voorspelde waarden} $\hat{y}_i$.

\begin{equation}
\text{MAE} = \frac{1}{n} \cdot \sum_{i=1}^n | y_i - \hat{y}_i |
\end{equation}
\end{theorem}

\begin{minted}{python}
from sklearn.metrics import mean_absolute_error

y_predicted = lgregmodel.predict(X_test)
MAE = mean_absolute_error(y_test, y_predicted)

print('MAE= ', MAE)

>> MAE = 64.0090867586

\end{minted}

\subsubsection{Mean Squared Error (MSE)}

\begin{theorem}[MSE]
De Mean Squared Error (MSE) is \textbf{het gemiddelde van de gekwadrateerde waarden} van het verschil tussen de werkelijke waarden $y_i$ en
de voorspelde waarden $\hat{y}_i$.

\begin{equation}
\text{MSE} = \frac{1}{n} \cdot \sum_{i=1}^n ( y_i - \hat{y}_i )^2
\end{equation}
\end{theorem}

\begin{minted}{python}
from sklearn.metrics import mean_squared_error

y_predicted = lregmodel.predict(X_test)
MSE = mean_squared_error(y_test, y_predicted)

print('MSE = ' MSE)

>> MSE = 7803.89332739
\end{minted}


\subsubsection{Determinatiecoëfficiënt}

\begin{theorem}[De determinatiecoëfficiënt $R^2$]
De determinatiecoëfficiënt ($R^2$) is de variabiliteit van het model

\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2 }{\sum_{i=1}^n (y_i - \bar{y})^2}
\end{equation}

Bij perfecte voorspellingen is $R^2 = 1$

Een negatieve waarde voor $R^2$ betekent dat het model slechter scoort dan een horizontale lijn (= slechter dan het gemiddelde te nemen) ($y_i = \bar{y}$, $\bar{y}$ is het gemiddelde van y)
\end{theorem}

\begin{minted}{python}
from sklearn.metrics import r2_score

y_predicted = lregmodel.predict(X_test)
r2 = r2_score(y_test, y_predicted)

print('r2_score = ', r2)

# alternatieve manier voor het bepalen van de r2 score:
r2 = lregmodel.score(X_test, y_test)

>> r2 score = 0.754254234917
\end{minted}

\section{Feature engineering}

Om een beter model te verkrijgen (en zo een betere $R^2$ score), kunnen we verschillende dingen doen:

\begin{itemize}
    \item Meer data toevoegen
    \item Ander model kiezen, hyperparameter tuning
    \item Feature engineering 
\end{itemize}

\subsection{Normalisatie / Scaling}

\begin{theorem}[Normalisatie / Scaling]
Normalisatie of Scaling zorgt ervoor dat de features op dezelfde schaalverdeling staan
\end{theorem}

In ons voorbeeld van de huurprijs:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{normalisatie.png}
    \caption{NOX: min = 0.385, max = 0.871; TAX: min = 188, max = 711}
\end{figure}

\subsubsection{Voordelen}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{scaling.png}
    \caption{Gradient Descent convergeert minder snel als features op een verschillende schaalgrootte staan. Normalisatie zorgt er dus voor dat het model sneller zal trainen.}
\end{figure}

\subsubsection{MIN-MAX-scaling}

\begin{equation}
x_{s_i} = \frac{x_i - Min(x)}{Max(x) - Min(x)}
\end{equation}

\begin{itemize}
    \item Schaalt alle features tussen 0 en 1
    \item Werkt goed bij niet-Gaussiaanse distributies en bij kleine variantie
    \item De scheefheid (skew) blijft bewaard
    \item Gevoelig voor uitschieters
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{min-max-scaling.png}
    \caption{Voor en na scaling}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{min-max-houses.png}
    \caption{Bij het voorbeeld van de huizenprijzen}
\end{figure}

\begin{minted}{python}
from sklean.preprocessing import MinMaxScaler

scaler = MinMaxScaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# alternatief
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
\end{minted}

\subsubsection{Standard scaling (normalisatie)}

\begin{equation}
x_{s_i} = \frac{x_i - mean(x)}{stdev(x)}
\end{equation}

\begin{itemize}
    \item Geschaalde features;
    \begin{itemize}
        \item Gemiddelde = 0
        \item Standaardafwijking = 1
    \end{itemize}
    \item Geschaalde features schommelen rond 0 (soms nodig bij deep learning)
    \item Vervormt geen relatieve afstanden tussen de feature waarden
    \item Kan beter overweg met uitschieters
    \item Garandeert geen genormaliseerde data op exact dezelfde schaal
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{standard-scaling.png}
    \caption{Voor en na standard scaling}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{standard-scaling-houses.png}
    \caption{Bij het voorbeeld van de huizenprijzen}
\end{figure}


\begin{minted}{python}
from sklean.preprocessing import StandardScaler

scaler = StandardScaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# alternatief
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
\end{minted}

\subsubsection{Robust scaling}

\begin{equation}
x_{s_i} = \frac{x_i - Q_2(x)}{Q_3(x) - Q_1(x)}
\end{equation}

\begin{itemize}
    \item Lijkt op MIN-MAX scaler maar gebruikt de interkwartielafstand ipv range
    \item Houdt geen rekening met uitschieters
    \item Gebruikt minder data bij het bepalen van de schaal
    \item Range van de genormaliseerde data is groter dan bij MIN-MAX scaling
    \item Garandeert geen genormaliseerde data op exact dezelfde schaal
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{robust-scaling.png}
    \includegraphics[width=0.45\textwidth]{robust-scaling2.png}
    \caption{Voor en na robust scaling}
\end{figure}

\begin{minted}{python}
from sklean.preprocessing import RobustScaler

scaler = RobustScaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# alternatief
scaler = RobustScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
\end{minted}

\subsection{Feature expansion}

\subsubsection{Nieuwe features}

Bedenken van nieuwe features

\textbf{Voorbeelden:}

\begin{itemize}
    \item Uit de lengte en breedte van een huis de oppervlakte halen als nieuwe feature.
    \item Uit een start en eindpunt de afstand halen als nieuwe feature.
    \item Uit een datum afleiden welke dag van de week het is.
    \item Veranderingen in de features.
    \item Nieuwe opgemeten parameters.
\end{itemize}

\subsubsection{Hogere-orde features}

= Het verband tussen features en de target(s) is niet altijd lineair.

\textbf{Voorbeeld:} samenhang tussen LSTAT $(x_1)$ en de huizenprijs $(P)$

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{hogere-orde-features.png}
    \caption{$P = \theta_0 + \theta_1x_1$}
\end{figure}

Toevoegen van een extra hogere-orde feature $x_2 = x^2_1$:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{hogere-orde-features2.png}
    \caption{$P = \theta_0 + \theta_1x_1 + \theta_2x_2$ }
\end{figure}

Toevoegen van een extra hogere-orde features $x_3 = x^3_1$ en $x_4 = x^4_1$:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{hogere-orde-features3.png}
    \caption{$P = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3$ }
    \includegraphics[width=0.3\textwidth]{hogere-orde-features4.png}
    \caption{$P = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4$}
\end{figure}

\begin{figure}[H]
    \centering
\end{figure}

\begin{minted}{python}
# toevoegen van een extra feature: LSTAT^2 LSTAT^3
dataset.insert(dataset.columns.size - 1, 'LSTAT^2', dataset.LSTAT**2)
dataset.insert(dataset.columns.size - 1, 'LSTAT^3', dataset.LSTAT**3)
\end{minted}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{hogere-orde-features5.png}
    \caption{Resultaat toevoegen hogere-orde features}
\end{figure}

\begin{itemize}
    \item Nu model met extra features trainen en nadien testen op de test set.
    \item \textbf{Opgepast:} bij de test set moet je ook dezelfde features toevoegen.
\end{itemize}

\subsection{One-hot encoding}

\begin{itemize}
    \item = Omzetten van categorische variabelen naar meerdere aparte features
    \item categorische variabelen = variabelen zonder echte waarden, de waarden stellen een categorie voor (niet altijd een nummer)
    \item $\Rightarrow$ voor elke categorie een nieuwe kolom
    \item `Dummy Variable Trap'
    \begin{itemize}
        \item = als een rij maar tot 1 categorie kan behoren (zie onderstaand voorbeeld, een appel is geen kip), dan zou je in principe 1 kolom kunnen schrappen en dan kan je toch dezelfde informatie krijgen.
        \item $x_1 + x_2 + x_3 = 1 \Leftrightarrow x_1 = 1 - x_2 - x_3$   
        \item In de praktijk laat men dit gewoon staan.
        \item (niet te kennen op examen)
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{one-hot-encoding.png}
    \caption{Voorbeeld One-hot encoding}
\end{figure}

\begin{minted}{python}
# voeg de categorieen toe als kolommen
dataset = pd.concat(
    [dataset, pd.get_dummies(dataset['food_name'], prefix='food')], 
    axis=1)
# verwijder de food_name kolom
dataset.drop(['food_name'], axis=1, inplace=True)
# toon de eerste 5 rijen:
dataset.head()
\end{minted}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{one-hot-encoding2.png}
    \caption{Resultaat van dataset.head(), voor en na one-hot encoding}
\end{figure}

\section{Underfitting \& overfitting}

\subsection{Underfitting}

\begin{theorem}[Underfitting]
Underfitting treedt op wanneer een model de training data niet kan modelleren 
en ook niet kan generaliseren op nieuwe data.

\begin{itemize}
    \item Het model is te 'simpel'
    \item Model met hoge bias
\end{itemize}
\end{theorem}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{underfitting.png}
    \caption{Het model werkt voor sommige variabelen maar voor velen ook niet}
\end{figure}

\subsection{Overfitting}

\begin{theorem}[Overfitting]
Overfitting treedt op wanneer een model de training data te goed modelleert en niet kan
    generaliseren op nieuwe data.  

\begin{itemize}
    \item Het model is te 'complex'
    \item De ruis van willekeurige fluctuaties in data worden opgepikt
    \item Model met een hoge variance
\end{itemize}
\end{theorem}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{overfitting.png}
    \caption{Model met order $\approx 15$}
\end{figure}

\subsubsection{Impact van de grootte van de dataset}

Afhankelijkheid van de grootte van de dataset (aantal observaties $m$):

\begin{itemize}
    \item Bij weinig observaties: snel overfitting bij complexer model
    \item Bij veel observaties: minder snel overfitting bij complexer model
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{overfitting2.png}
    \caption{Voorbeeld: 3de orde polynoom}
\end{figure}

\subsection{Regularisatie (regularisation)}

\begin{theorem}[Regularisatie]
Methode om de mate van bias van een hypothese te regelen en een 
goed evenwicht te vinden tussen underfitting en overfitting.

\begin{equation}
J(\theta) = \frac{1}{2\cdot m} \sum_{i=1}^m (h_{\theta}(x_i) - y_i)^2 + R(\theta)
\end{equation}


\end{theorem}

\begin{theorem}[$R(\theta)$]
$R(\theta)$ is de regularisatie-term. 

Dit is een extra kostenterm die het gebruik van hogere orde features afstraft
tenzij ze de globale kostenfunctie doen dalen.
\end{theorem}

\subsubsection{Voorbeeld regularisatie}

\begin{center}
    $J(\theta) = \frac{1}{2\cdot m} \sum_{i=1}^m (h_{\theta}(x_i) - y_i)^2 + \lambda \theta \theta^{\tau}$
\end{center}

\begin{itemize}
    \item $\theta = \{\theta_1, \dots, \theta_n\}$
    \begin{itemize}
        \item intercept $\theta_0$ wordt meestal niet geregulariseerd
    \end{itemize}
    \item $\lambda$ is een tuning parameter (hyper parameter), we moeten die zelf vinden
    \begin{itemize}
        \item $\lambda = 0 \Rightarrow$ geen regularisatie
        \item $\lambda = \inf \Rightarrow \theta = 0$ 
        \item $\lambda$ tussenin regelt de mate van regularisatie.
    \end{itemize}
\end{itemize}

De tuningparameter $\lambda$ regelt de complexiteit van de hypothese:

\begin{itemize}
    \item Kleine waarde voor $\lambda$: lage bias, hoge variantie (overfitting)
    \item Grote waarde voor $\lambda$: hoge bias, lage variantie (underfitting)
\end{itemize}

Afhankelijk van hoe $R$ gedefinieerd wordt is er een andere benaming voor de regularisatie:

\begin{itemize}
    \item Ridge regression (L2 regularisatie)
    \item Lasso regression (L1 regularisatie)
\end{itemize}

\subsubsection{Regularisatie met L2 norm}

\begin{center}
$J_{L2} = J + \lambda_2 \sum_{j=1}^m \theta_j^2$

$J_{L2} = \sum_{i=1}^n (\text{target}_i - \text{output}_i) + \lambda_2 \sum_{j=1}^m \theta_j^2$
\end{center}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{regularisatie-l2.png}
    \caption{}
\end{figure}

\subsubsection{Regularisatie met L1 norm}

\begin{center}
$J_{L1} = J + \lambda_1 \sum_{j=1}^m |\theta_j|$

$J_{L1} = \sum_{i=1}^n (\text{target}_i - \text{output}_i) + \lambda_1 \sum_{j=1}^m |\theta_j|$
\end{center}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{regularisatie-l1.png}
    \caption{}
\end{figure}

\subsubsection{Voorbeeld regularisatie op huizenprijzen}

Via Ridge of Lasso regressie met regularisatieparameter $\alpha$

\begin{itemize}
    \item Hoe groter $\alpha$, hoe sterker de regularisatie en dus hoe simpler het model
    \item Hoe kleiner $\alpha$, hoe zwakker de regularisatie en dus hoe complexer het model
\end{itemize}

\begin{minted}{python}
regmodel = Ridge(alpha=0.14, tol=0.0001, fit_intercept=True)
regmodel.fit(X_train, y_train)
regmodel.score(X_test, y_test)
>> 0.79834480089914472

lregmodel = Lasso(alpha=0.5, tol=0.0001, fit_intercept=True)
lregomdel.fit(X_train, y_train)
lregmodel.score(X_test, y_test)
>> 0.8437113338085345
\end{minted}

\section{Classificatie}

\subsection{Wat is classificatie?}

\begin{theorem}[Classificatie]
Classificatie is een supervised learning techniek waarbij een getraind model
niet geziene inputs toewijst aan één of meerdere gelabelde categorieën (classes)
\end{theorem}

\subsubsection{Voorbeelden}

\begin{itemize}
    \item Gezichtsherkenning
    \item Nummerplaatherkenning
    \item Spam detectie
    \item Medische diagnoses
    \item Voorspelling of een klant op een advertentie zal klikken
    \item Kwaliteitscontrole
    \item \dots
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{classificatie-gezichtsherkenning.png}
    \caption{Geslachtsherkenning + leeftijdsclassificatie}
\end{figure}


\subsection{Types van classifiers}

\subsubsection{Binary (binomial) classifier}

= verdeel de samples in \textbf{twee verschillende klasses}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{binary-class.png}
    \caption{Voorbeeld: bepaal of een kanker goedaardig of kwaadaardig is}
\end{figure}

\subsubsection{Multiclass classifier}

Verdeel de samples in drie of meerdere verschillende klasses

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multiclass-classifier.png}
    \caption{Voorbeelden: gezichtsherkenning, sentiment analyse}
\end{figure}

\subsubsection{Multilabel classifier}

Er kunnen meerdere labels aan een sample toegewezen worden. 
Een sample kan tot meerdere klasses behoren

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multilabel-classifier.png}
    \caption{Image content analysis, een film kan tot meerdere genres behoren}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{multilabel-classifier2.png}
    \caption{Voorspel of een stuk fruit een appel is op basis van vorm en kleur}
\end{figure}

\begin{itemize}
    \item features: rondheid en groenheid
    \item target: appel: ja/nee
    \item trainingset: 15 training samples
\end{itemize}

Bij classificatie is de output/target een \textbf{(discrete) variabele/klasse}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multilabel-classifier3.png}
    \caption{}
\end{figure}

\subsection{Waarom regressie geen goede optie is}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{classificatie-regressie.png}
    \caption{Voorbeeld: rondheid bepaalt of het een appel is of niet}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{classificatie-regressie2.png}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{classificatie-regressie3.png}
    \caption{}
\end{figure}

TODO: meer uitleg

\section{Logistische regressie}

\subsection{Het model}

We willen dat het model $h_{\theta}(x)$ voldoet aan:

\begin{center}
$0 \leq h_{\theta}(x) \leq 1$
\end{center}

\begin{itemize}
    \item $h_{\theta}(x)$ = de geschatte kans dat $y=1$ bij input $x$
    \item Voorbeeld: $h_{\theta}(x) = 0.80$
    \begin{itemize}
        \item Het model is voor 80\% zeker dat het om een appel gaat.
    \end{itemize}
\end{itemize}

\begin{equation}
h_{\theta}(x) = \frac{1}{1 + e^{-\theta^{\tau}x}}
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{logistic-regression.png}
    \caption{}
\end{figure}

\begin{itemize}
    \item $y=1$ als $h_{\theta}(x) \geq 0.5 \Rightarrow \theta^{\tau}x \geq 0$
    \item $y=0$ als $h_{\theta}(x) < 0.5 \Rightarrow \theta^{\tau}x < 0$
\end{itemize}

\subsubsection{Interpretatie via voorbeeld appels}

Het model is van de vorm:

\begin{center}
    $h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2)$

    met $x_1=$ rondheid en $x_2=$ groenheid
\end{center}

Veronderstel na training: $\theta_0=-40, \theta_1=4, \theta_2=4$

\begin{itemize}
    \item Voorspel $y=1$ als $-40 + 4x_1 + 4x_2 \geq 0$
    \item Voorspel $y=0$ als $-40 + 4x_1 + 4x_2 < 0$
\end{itemize}

Gegeven een rondheid van 8 en een groenheid van 6:

\begin{center}
    $-40 + 4 \cdot 8 + 4 \cdot 6 = 16 \Rightarrow $ Appel

    $h_{\theta}(x) = \frac{1}{1 + e^{-16}} = 0.999999887$
\end{center}

Gegeven een rondheid van 5 en een groenheid van 4.5:

\begin{center}
    $-40 + 4 \cdot 5 + 4 \cdot 4.5 = -2 \Rightarrow $ Geen appel

    $h_{\theta}(x) = \frac{1}{1 + e^{-2}} = 0.12$
\end{center}

\begin{itemize}
    \item Het model is maar voor 12\% zeker dat het om een appel gaat
    \item $\Rightarrow$ Met 88\% zekerheid gaat het volgens het model niet om een appel
\end{itemize}

\subsubsection{Grafische interpretatie via voorbeeld appels}

\begin{itemize}
    \item Op de scheidingslijn: $\theta_0 + \theta_1x_1 + \theta_2x_2 = 0$
    \item In het voorbeeld: $-40 + 4x_1 + 4_x2 = 0$
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{logistic-grafische.png}
    \caption{}
\end{figure}

\subsubsection{Wat als het model niet lineair is?}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{logistic-circle.png}
    \caption{Het model is niet lineair}
\end{figure}


Extra features: $h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1^2 + \theta_4x_4^2)$

\begin{itemize}
    \item Veronderstel: $\theta_0 = -2, \theta_1 = 0, \theta_2 = 0, \theta_3 = 1, \theta_4 = 1$
    \item Voorspel $y=1$ als $-2 + x_1^2 + x_2^2 \geq 0$
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{logistic-circle2.png}
    \caption{$x_1^2 + x_2^2 \geq 2$ (vergelijking van een cirkel met straal $\sqrt{2}$)}
\end{figure}

\subsection{De kostenfunctie}

De kostenfunctie wordt:

\begin{equation}
J(\theta) = \left\{
    \begin{array}{ll}
        -ln(h_\theta(x))\ \text{als}\ y = 1\\
        -ln(1 - h_\theta(x))\ \text{als}\ y = 0\\
    \end{array}
\right.
\end{equation}

\begin{equation}
J(\theta) = -\frac{1}{m} \cdot \Bigg[ \sum_{i=1}^m y_i ln(h_{\theta}(x_i)) + (1 - y_i) \cdot ln(1 - h_{\theta}(x_i)) \Bigg]
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{logistic-kostenfunctie.png}
    \caption{Zoek de waarden voor $\theta$ die de kostenfunctie $J(\theta)$ minimaliseert via Gradient Descent (GDS)}
\end{figure}

\subsection{Voorbeeld logistic regression met Sklearn (lineair model)}

\subsubsection{Preprocessing van de data}

Analoog aan preprocessing bij lineaire regressie:

\begin{itemize}
    \item Data inlezen
    \item Check op inconsistenties
    \item Check uitschieters
    \item Plot de data
    \item Splits op in features en targets
    \item Verdeel in een training en test set
\end{itemize}

\begin{minted}{python}
# importeer de dataset
dataset = pd.read_csv('appels.csv')
# definieer de features
features = list(dataset.columns[:dataset.columns.size-1])
X = dataset[features].values
y = dataset['appel'].values
# lettergrootte voor de axis labels
sns.set(font_scale=2) 
# definieer kleuren
colors = ['blue', 'red', 'greyish', 'faded_green', 'dusty_purple']
# plot
sns.lmplot(x='rondheid', y='groenheid', data=dataset, 
    fit_reg=False,  hue='appel', palette=sns.xkcd_palette(colors), 
    scatter_kws={'s': 500}, size=7, aspect=1.5)
\end{minted}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{logistic-sklearn-preprocessing.png}
    \caption{Plot van de dataset}
\end{figure}


\subsubsection{Trainen van het logistic regression model}

\begin{minted}{python}
# train een logistic regression classifier
logreg = linear_model.LogisticRegression(C=1e5)
# C = inverse of regularisation strength
# must be a positive float
# like in support vector machines: smaller values => stronger regularisation
logreg.fit(X, y)

print('coefficienten: ', logref.coef_)
print('intercept: ', logreg.intercept_)

\end{minted}

\begin{itemize}
    \item Coefficienten: $\theta_1 = 4.287$ en $\theta_2 = 4.062$
    \item Intercept: $\theta_0 = -43.941$
\end{itemize}

\subsubsection{Classificeren van een nieuwe sample}

\begin{minted}{python}
# voorspel de klasse met rondheid = 8 en groenheid = 6
print(logreg.predict(np.array([8,6]).reshape(1, -1)))

kans = logreg.predict_proba(np.array([8,6]).reshape(1, -1))
print("Kans op appel/geen appel = ", kans)

# voorspel de klasse met rondheid = 4 en groenheid = 4
print(logreg.predict(np.array([4,4]).reshape(1, -1)))

kans = logreg.predict_proba(np.array([4,4]).reshape(1, -1))
print("Kans op appel/geen appel = ", kans)
\end{minted}

Output:

\begin{itemize}
    \item kans op een appel/geen appel: 3.99395302e-07 | 9.99999601e-01
    \item kans op een appel/geen appel: 9.99973583e-01 | 2.64168196e-05
\end{itemize}

\subsubsection{Visualiseer de decision boundary}

\begin{minted}{python}
h = 0.01
rond_min = X[:,0].min()-2
rond_max = X[:,0].max()+2
groen_min = X[:,1].min()-2
groen_max = X[:,1].max()+2
xx, yy = np.meshgrid(np.arange(rond_min, rond_max, h),np.arange(groen_min, groen_max, h))
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# font settings
sns.set(font_scale = 2)
# colors
colors = ["blue", "red", "greyish", "faded green", "dusty purple"]
sns.lmplot(x='rondheid',y='groenheid',data=dataset,
    fit_reg=False,hue='appel',palette=sns.xkcd_palette(colors),
    scatter_kws={'s':200}, height=8, aspect=1.5)
plt.ylim(0, 11)
plt.xlim(0, 11)
plt.contour(xx, yy, Z, colors='green')
\end{minted}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{logistic-sklearn-decision-boundary.png}
    \caption{Visualisatie van de decision boundary}
\end{figure}

\subsection{Voorbeeld logistic regression met Sklearn - niet-lineair}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{logistic-non-linear.png}
    \caption{Niet-lineair scheidbare dataset}
\end{figure}

\subsubsection{Feature engineering: toevoegen van hogere orde features}

\begin{minted}{python}
# Aanmaken van de hogere orde features
graad = 3

poly = PolynomialFeatures(graad)
Xp = poly.fit_transform(X)

# Train model op hogere orde features en visualiseer de decision boundary
logreg_poly = linear_model.LogisticRegression(C=1)
logreg_poly.fit(Xp, y)
\end{minted}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{logistic-non-linear2.png}
    \caption{Resultaat: overfitting}
\end{figure}


\subsubsection{Oplossing: regularisatie}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{logistic-fitting.png}
    \caption{Regelen tussen underfitting en overfitting via regularisatie}
\end{figure}

\begin{itemize}
    \item Via regularisatie een goed evenwicht zoeken tussen underfitting en overfitting
    \item In Scikit Learn linear\_model.LogisticRegression:
    \begin{itemize}
        \item C = inverse regularisatie sterkte
        \item kleine waarden voor C $\Rightarrow$ sterke regularisatie (overfitting)
        \item grote waarden voor C $\Rightarrow$ zwakke regularisatie (underfitting)
    \end{itemize}
\end{itemize}

\begin{minted}{python}
logreg = linear_model.LogisticRegression(C=100)
logreg.fit(Xf, y)
\end{minted}

\subsection{Multi-class classification}

Data kan tot meerdere klasses behoren.
We kunnen met zo'n data dus geen binaire classificatie doen.


\subsubsection{One-vs-all}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multi-class-classification.png}
    \caption{One-vs-All: 3 binaire classifiers}
\end{figure}


\begin{itemize}
    \item "One-vs-Rest"
    \item Je classificeert 1 klasse tegen de rest
    \item Je hebt dus 1 classifier per klasse
    \item Totaal aantal classifiers: N
    \item Gevoeliger voor niet-gebalanceerde data (als er in een bepaalde klasse weinig datasamples zijn)
\end{itemize}

\subsubsection{One-vs-One}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{multi-class-classification2.png}
    \caption{One-vs-One}
\end{figure}

\begin{itemize}
    \item Je hebt $\frac{N\cdot(N-1)}{2}$ classifiers (met N aantal klasses) $\Rightarrow$ rekenintensief
    \item Minder gevoelig voor niet-gebalanceerde data
\end{itemize}

\section{Evaluatie van een classifier}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{truth-table.png}
    \caption{Confusion matrix}
\end{figure}

Stel: we hebben een binaire classifier die klasse 0 of 1 als output heeft.
We vergelijken de verwachte waarden uit de classifier met de werkelijke waardes uit de testset.

\begin{itemize}
    \item True Positive = de verwachte waarde en werkelijke waarde zijn beide 1
    \item False Positive = de verwachte waarde is 1, terwijl de werkelijke waarde 0 is
    \item False Negative = de verwachte waarde is 0, terwijl de werkelijke waarde 1 is
    \item True Negative = de verwachte waarde en werkelijke waarde zijn beide 0
\end{itemize}

We tellen het aantal TP, FP, FN, TN om te gebruiken in de volgende formules. 
Deze formules kunnen we gebruiken om een classifier te evalueren.

\subsection{Accuracy}

\begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN}
\end{equation}

\begin{itemize}
    \item Probleem: stel dat TN = 990, FN = 10, TP = 0, FP = 0
    \item Dan heb je een accuracy van 99\%, terwijl je eigenlijk geen enkele positieve predictie
    \item Accuracy is dus niet altijd veelzeggend $\Rightarrow$ nood aan andere termen
\end{itemize}

\subsection{True Positive Rate (TPR)}

= Recall = Sensitivity = Hit rate

\begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\begin{itemize}
    \item Wordt vaak in combinatie gebruikt met Precision (PPV)
\end{itemize}

TODO

\subsection{Positive Predictive Value (PPV)}

= Precision

\begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\subsection{F1 Score}

\begin{equation}
    \text{F1 score} = \frac{2\cdot(\text{Recall} \cdot \text{Precision})}{(\text{Recall} + \text{Precision})}
\end{equation}

\begin{itemize}
    \item Is een harmonisch gewogen gemiddelde van de recall en de precision
\end{itemize}

\subsection{Receiver Operating Characteristic (ROC)}

\begin{itemize}
    \item Wordt gebruikt bij binaire classifiers (2 klasses)
    \item Wordt gebruikt om beter modellen te selecteren en minder goede te verwerpen
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{roc.png}
    \includegraphics[width=0.3\textwidth]{roc2.png}
    \caption{ROC Curve (x-as = FPR, y-as = TPR)}
\end{figure}

We gebruiken voor de assen de TPR en FPR

\begin{itemize}
    \item True Postitive Rate = Sensitivity = Recall (zie hierboven)
    \item False Positive Rate = $1 - \text{Specificity} = \frac{FP}{FP + TN}$
    \item Specificity = $\frac{TN}{FP + TN}$
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{roc3.png}
    \caption{Classifier C' is een betere classifier dan classifier A}
\end{figure}

\begin{itemize}
    \item Bij classifier C' hebben we het omgekeerde van C genomen
    \item We noemen linksboven `ROC heaven' en rechtsbeneden `ROC Hell'.
    \item De rode stippelijn is een classifier die gewoon gokt.
    \item B is een classifier die even goed is als een gewone gok.
\end{itemize}


\subsubsection{ROC curve en AUC (Area Under ROC Curve)}

ROC bij verschillende treshold settings: 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.15\textwidth]{roc-treshold.png}
    \includegraphics[width=0.25\textwidth]{roc-treshold2.png}
    \caption{$p(y=1 | x) - \text{threshold} = 0.5$ }
\end{figure}

\begin{itemize}
    \item Testset met 10 samples
    \item $TPR = \frac{TP}{TP + FN} = \frac{4}{4 + 1} = 0.8$
    \item $FPR = \frac{FP}{FP + TN} = \frac{3}{3 + 2} = 0.6$
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.15\textwidth]{roc-treshold3.png}
    \includegraphics[width=0.25\textwidth]{roc-treshold4.png}
    \caption{$p(y=1 | x) - \text{threshold} = 0.6$ }
\end{figure}

\begin{itemize}
    \item Met $p(y=1 | x) - \text{threshold} = 0.6$ krijgen we dezelfde TRP, maar een betere FPR
    \item $TPR = \frac{TP}{TP + FN} = \frac{4}{4 + 1} = 0.8$
    \item $FPR = \frac{FP}{FP + TN} = \frac{0}{0 + 5} = 0$
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{AuROC.png}
    \caption{AOC of AuROC = Area under ROC}
\end{figure}

\begin{itemize}
    \item Stel dat de treshold 0 is:
    \begin{itemize}
        \item Dan is TPR = 100\%
        \item Maar FPR is ook 100\%
    \end{itemize}
    \item Stel dat de treshold dichter bij 1 komt (bv: 0.95):
    \begin{itemize}
        \item Dan is TPR dichter bij 0\%
        \item En FPR ook dichter bij 0\%
    \end{itemize}
    \item Je kiest de beste ROC $\Rightarrow$ ROC met grootste AOC
\end{itemize}



\end{document}